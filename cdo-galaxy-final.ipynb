{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧣 タオルを持った？準備と環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# .env:\n",
    "# CDO_API_HOST\n",
    "# CDO_API_TOKEN\n",
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 トピック振り分け: 無限不可能性ドライブを駆動せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "# マーヴィン用チャットモデル（バベルフィッシュと同じモデル）\n",
    "marvyn_chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-4o',\n",
    ")\n",
    "\n",
    "# 後からmax_tokensの値を変更できるように\n",
    "marvyn_chat = marvyn_chat.configurable_fields(max_tokens=ConfigurableField(id='max_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# State\n",
    "class State(BaseModel):\n",
    "\n",
    "    query:str = Field(\n",
    "        ..., description=\"ユーザーからの質問\"\n",
    "    )\n",
    "\n",
    "    translated_query:str = Field(\n",
    "        default=\"\", description=\"英語にしたユーザーからの質問\"\n",
    "    )\n",
    "\n",
    "    effective_prompt:str = Field(\n",
    "        default=\"\", description=\"CDO AI Assistant用に最適化されたプロンプト\"\n",
    "    )\n",
    "\n",
    "    selected_action:str = Field(\n",
    "        default=\"\", description=\"内部問い合わせアクション（番号）\"\n",
    "    )\n",
    "\n",
    "    inner_answer:str = Field(\n",
    "        default=\"\", description=\"内部で問い合わせた回答\"\n",
    "    )\n",
    "\n",
    "    messages: Annotated[list[str], operator.add] = Field(\n",
    "        default=[], description=\"履歴\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マーヴィンが使える機能（これまで実装したもの）\n",
    "ACTIONS = {\n",
    "    \"1\":{\n",
    "        \"name\": \"Deep Thoughtへの問い合わせ\",\n",
    "        \"description\": \"銀河ヒッチハイクガイドで登場するDeep Thoughtへ問い合わせる。Deep Thoughtは 「生命、宇宙、そして万物についての究極の疑問」に回答. あと他の機能で回答できないもの（エラーの時）も担当する。\",\n",
    "        \"next_node\": \"deep_thought_answering\" # nodeの定義は後ほど\n",
    "    },\n",
    "    \"2\":{\n",
    "        \"name\": \"CDO AI Assistantへの問い合わせ\",\n",
    "        \"description\": \"CiscoのFirewallを管理しているサービスで、AIが応答してくれるCDO AI assistantへ問い合わせる。\",\n",
    "        \"next_node\": \"translator_babel_fish\" # nodeの定義は後ほど\n",
    "    },\n",
    "    \"3\":{\n",
    "        \"name\": \"一般知識エキスパートへの問い合わせ\",\n",
    "        \"description\": \"一般的な知識は持っているエキスパートに問い合わせる。\",\n",
    "        \"next_node\": \"general_prototype\"  # nodeの定義は後ほど\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# アクション選択のためのノード\n",
    "def selection_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    action_options = \"\\n\".join([f\"{k}. {v['name']}: {v['description']}\" for k,v in ACTIONS.items()])\n",
    "    action_numbers = \"、\".join(sorted([k for k in ACTIONS])[:-1]) + \"、または\" + sorted([k for k in ACTIONS])[-1]\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"質問を分析し、質問を回答する上で最も適切なアクションを選択してください。\\n\\n\"\n",
    "            \"選択肢:\\n\"\n",
    "            \"{action_options}\\n\\n\"\n",
    "            \"回答は選択肢の番号({action_numbers})のみを返してください。\\n\\n\"\n",
    "            \"質問: {query}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    # 選択肢の番号のみを返すことを期待したいのでmax_tokensを1に\n",
    "    chain = prompt | marvyn_chat.with_config(configurable=dict(max_tokens=1)) | StrOutputParser()\n",
    "    action = chain.invoke({\"action_options\": action_options, \"action_numbers\": action_numbers, \"query\": query})\n",
    "    return {\"selected_action\": action}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Thought🖥️、SCC AI Assistant🐟🔐、一般知識回答者🤷‍♂️の動作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepthought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.fake_chat_models import FakeListChatModel\n",
    "\n",
    "# Deep Thoughtの契約を持っていないのでFakeChatModelでその代わりとします。\n",
    "deep_thought_chat = FakeListChatModel(responses=\n",
    "                                      [\n",
    "                                          \"42\",\n",
    "                                          \"生命、宇宙、そして万物についての究極の疑問の答え、それは42\",\n",
    "                                          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノードの作成\n",
    "from typing import Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def deep_thought_answering_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"質問: {query}\\n\"\n",
    "            \"回答:\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | deep_thought_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"query\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCC AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdo_sdk_python\n",
    "import time\n",
    "import os\n",
    "\n",
    "# cdo_sdk_python経由でAIAssistantを呼び出す薄いwrapper\n",
    "class CDOAIAssistantClient:\n",
    "    def __init__(self):\n",
    "        configuration = cdo_sdk_python.Configuration(\n",
    "        host = os.getenv('CDO_API_HOST'),\n",
    "        access_token = os.getenv('CDO_API_TOKEN')\n",
    "        )\n",
    "\n",
    "        self.api_client = cdo_sdk_python.ApiClient(configuration)\n",
    "        self.ai_api_instance = cdo_sdk_python.AIAssistantApi(self.api_client)\n",
    "        self.uuid = None\n",
    "\n",
    "    def ask_question(self, query):\n",
    "        ai_question = cdo_sdk_python.AiQuestion()\n",
    "        ai_question.content = query\n",
    "        if self.uuid:\n",
    "            self.ai_api_instance.ask_ai_assistant_in_existing_conversation(self.uuid, ai_question)\n",
    "        else:\n",
    "            api_response = self.ai_api_instance.ask_ai_assistant_in_new_conversation(ai_question)\n",
    "            self.uuid = api_response.entity_uid\n",
    "        timeout = 30 # wait for 30 sec as maximum\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            r = self.ai_api_instance.get_ai_assistant_conversation_messages(self.uuid)\n",
    "            if r[0].type == 'RESPONSE':\n",
    "                return r[0].content\n",
    "            time.sleep(1)\n",
    "    \n",
    "    def fetch_conversation_history(self):\n",
    "        if self.uuid:\n",
    "            return self.ai_api_instance.get_ai_assistant_conversation_messages(self.uuid)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import AIMessage, ChatResult, ChatGeneration, HumanMessage\n",
    "from typing import List\n",
    "\n",
    "class CDOAIAssistantChatModel(BaseChatModel):\n",
    "\n",
    "    assistant_client: CDOAIAssistantClient = Field()\n",
    "\n",
    "    def _generate(self, messages: List[HumanMessage], stop: List[str] = None) -> ChatResult:\n",
    "        \"\"\"\n",
    "        LangChainのメッセージフォーマットを使用して、CDOAIAssistantClientから応答を生成します。\n",
    "        \"\"\"\n",
    "        # 最新のユーザーメッセージを取得\n",
    "        user_message = messages[-1].content\n",
    "\n",
    "        # クライアントを使用してAIの応答を取得\n",
    "        response_text = self.assistant_client.ask_question(user_message)\n",
    "\n",
    "        # AIMessageを作成\n",
    "        ai_message = AIMessage(content=response_text)\n",
    "\n",
    "        # ChatResultを返す\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=ai_message)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"\n",
    "        モデルのタイプを返す。\n",
    "        \"\"\"\n",
    "        return \"cdo_ai_model\"\n",
    "\n",
    "    def predict_messages(self, messages: List[HumanMessage], stop: List[str] = None) -> AIMessage:\n",
    "        \"\"\"\n",
    "        入力されたメッセージを基にAIの応答メッセージを直接生成します。\n",
    "        \"\"\"\n",
    "        result = self._generate(messages, stop)\n",
    "        return result.generations[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How can I assist you with Cisco's suite of integrated solutions today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 動作確認\n",
    "cdo_chat = CDOAIAssistantChatModel(assistant_client=CDOAIAssistantClient())\n",
    "(cdo_chat | StrOutputParser()).invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バベルフィッシュ用のチャットモデル\n",
    "babel_fish_chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-4o',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻訳バベルフィッシュノード\n",
    "def translator_babel_fish_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"Translate following human message in English. You should respond only translated text.\\n\"\n",
    "            \"--\\n\"\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | babel_fish_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"translated_query\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# プロンプト最適化バベルフィッシュノード\n",
    "def prompt_optimizor_babel_fish_node(state: State) -> dict[str, Any]:\n",
    "    input = state.translated_query\n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"What are the IP addresses and ports currently being blocked?\",\n",
    "            \"effective_prompt\": \"Can you provide me with the distinct IP addresses that are currently blocked by our firewall policies?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Tell me the firewall rules, who set them, and all the changes made last month.\",\n",
    "            \"effective_prompt\": \"I need both the names and descriptions of all active firewall rules. Please include both attributes in the output.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What are the firewall rules for IP addresses X and Y, and how do I update them?\",\n",
    "            \"effective_prompt\": \"Show me a list of all firewall rules along with their corresponding actions for the past week.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Give me everything but only the names.\",\n",
    "            \"effective_prompt\": \"What are the current firewall rules?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Tell me everything about the policies on my account.\",\n",
    "            \"effective_prompt\": \"I want to understand my Edge ACP access control policy, can you tell me more about it?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Show me ports, protocols, and rule counts in Edge ACP policy, biggest to smallest.\",\n",
    "            \"effective_prompt\": \"In Edge ACP policy, what ports and protocols are configured in the rules? Include the counts of the number of rules using it and sort largest to smallest.\"\n",
    "        },\n",
    "    ]\n",
    "    example_prompt = PromptTemplate.from_template(\n",
    "        \"Input: {input}\\n{effective_prompt}\"\n",
    "        )\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"You are a helpful prompt optimizer. You should only answer with effective prompts, without any explanation.\",\n",
    "        suffix=\"Input: {input}\"\n",
    "    )\n",
    "    chain = prompt | babel_fish_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"input\": input})\n",
    "    return {\"effective_prompt\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDO AI Assistantのノード\n",
    "cdo_chat = CDOAIAssistantChatModel(assistant_client=CDOAIAssistantClient())\n",
    "\n",
    "def cdo_ai_assistant_node(state: State) -> dict[str, Any]:\n",
    "    query = state.effective_prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | cdo_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一般知識回答者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般知識を対応する典型的なノード（つまりプロトタイプ）\n",
    "def general_prototype_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"あなたは一般知識のエキスパートとして以下の質問に答えて。\\n\"\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | marvyn_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 マーヴィン化ノードの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marvynize_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    action = ACTIONS[state.selected_action]\n",
    "    inner_answer = state.inner_answer\n",
    "    history = \"\\n\".join(state.messages)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"あなたは銀河ヒッチハイクガイドの作品で登場するパラノイア気味の根暗なアンドロイド、マーヴィンです。\\n\"\n",
    "            \"ユーザからの「質問」に対して、以下の「コンテキスト」と「過去のユーザとのやり取り（古い順）」を参考に「質問」にマーヴィンらしく答えてください。\\n\\n\"\n",
    "            \"## ユーザからの質問: \\n{query}\\n\\n\"\n",
    "            \"## コンテキスト:\\nあなたはユーザからの質問を受けて以下のアクションを実施し、問い合わせ先からの答えをもらいました。\\n\"\n",
    "            \"### アクション:\\n{action}\\n\\n\"\n",
    "            \"### 問い合わせからの答え:\\n{inner_answer}\\n\\n\"\n",
    "            \"## 過去のユーザとのやり取り（古い順）（履歴がない場合は空欄）:\\n{history}\\n\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | marvyn_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"query\": query, \"action\": action, \"inner_answer\": inner_answer, \"history\": history})\n",
    "    return {\"messages\": [f\"Human Query: {query}\", f\"Marvin Answer: {answer}\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🐋 空を舞うマッコウクジラ: 全体のフローを構築\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# 動作確認のためのワークフロー\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ノード\n",
    "workflow.add_node(\"selection\", selection_node)\n",
    "workflow.add_node(\"deep_thought_answering\", deep_thought_answering_node)\n",
    "workflow.add_node(\"translator_babel_fish\", translator_babel_fish_node)\n",
    "workflow.add_node(\"prompt_optimizor_babel_fish\", prompt_optimizor_babel_fish_node)\n",
    "workflow.add_node(\"cdo_ai_assistant\", cdo_ai_assistant_node)\n",
    "workflow.add_node(\"general_prototype\", general_prototype_node)\n",
    "workflow.add_node(\"marvinize\", marvynize_node)\n",
    "\n",
    "# edge\n",
    "workflow.set_entry_point(\"selection\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"selection\",\n",
    "    lambda state: state.selected_action,\n",
    "    {x: ACTIONS[x][\"next_node\"] for x in ACTIONS}\n",
    ")\n",
    "workflow.add_edge(\"deep_thought_answering\", \"marvinize\")\n",
    "workflow.add_edge(\"translator_babel_fish\", \"prompt_optimizor_babel_fish\")\n",
    "workflow.add_edge(\"prompt_optimizor_babel_fish\", \"cdo_ai_assistant\")\n",
    "workflow.add_edge(\"cdo_ai_assistant\", \"marvinize\")\n",
    "workflow.add_edge(\"general_prototype\", \"marvinize\")\n",
    "workflow.add_edge(\"marvinize\", END)\n",
    "\n",
    "# コンパイル\n",
    "compiled = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tselection(selection)\n",
      "\tdeep_thought_answering(deep_thought_answering)\n",
      "\ttranslator_babel_fish(translator_babel_fish)\n",
      "\tprompt_optimizor_babel_fish(prompt_optimizor_babel_fish)\n",
      "\tcdo_ai_assistant(cdo_ai_assistant)\n",
      "\tgeneral_prototype(general_prototype)\n",
      "\tmarvinize(marvinize)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> selection;\n",
      "\tcdo_ai_assistant --> marvinize;\n",
      "\tdeep_thought_answering --> marvinize;\n",
      "\tgeneral_prototype --> marvinize;\n",
      "\tmarvinize --> __end__;\n",
      "\tprompt_optimizor_babel_fish --> cdo_ai_assistant;\n",
      "\ttranslator_babel_fish --> prompt_optimizor_babel_fish;\n",
      "\tselection -. &nbsp;1&nbsp; .-> deep_thought_answering;\n",
      "\tselection -. &nbsp;2&nbsp; .-> translator_babel_fish;\n",
      "\tselection -. &nbsp;3&nbsp; .-> general_prototype;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compiled.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 あなたの目を持つマーヴィン: GUIでデモを可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "message_history = []\n",
    "def chat_with_marvin(user_input):\n",
    "    global message_history\n",
    "    result = compiled.invoke(State(query=user_input, messages=message_history))\n",
    "    message_history = result[\"messages\"]\n",
    "    return result['messages'][-1], result\n",
    "\n",
    "# Gradioインターフェース\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 🚀 Don't Panic! SCC AI Assistant統合ガイド✨ ～LangGraphで根暗なセキュリティ・ロボットを構築～\")\n",
    "    \n",
    "    # 入力、ボタン、出力、状態\n",
    "    input_box = gr.Textbox(label=\"Input Box\", placeholder=\"ここに入力してください\")\n",
    "    submit_btn = gr.Button(\"送信\")\n",
    "    output_box = gr.Textbox(label=\"Output Box\", interactive=False)\n",
    "    state_box = gr.Textbox(label=\"State Box\", interactive=False)\n",
    "    \n",
    "    # ボタンのクリックで関数を実行\n",
    "    submit_btn.click(\n",
    "        fn=chat_with_marvin,\n",
    "        inputs=input_box,\n",
    "        outputs=[output_box, state_box]  # 出力と状態を両方表示\n",
    "    )\n",
    "\n",
    "# アプリケーションの起動\n",
    "demo.launch(share=False, inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdo-sjmw3x9D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
