{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§£ ã‚¿ã‚ªãƒ«ã‚’æŒã£ãŸï¼Ÿæº–å‚™ã¨ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# .env:\n",
    "# CDO_API_HOST\n",
    "# CDO_API_TOKEN\n",
    "# OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ€ ãƒˆãƒ”ãƒƒã‚¯æŒ¯ã‚Šåˆ†ã‘: ç„¡é™ä¸å¯èƒ½æ€§ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’é§†å‹•ã›ã‚ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "# ãƒãƒ¼ãƒ´ã‚£ãƒ³ç”¨ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ™ãƒ«ãƒ•ã‚£ãƒƒã‚·ãƒ¥ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ï¼‰\n",
    "marvyn_chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-4o',\n",
    ")\n",
    "\n",
    "# å¾Œã‹ã‚‰max_tokensã®å€¤ã‚’å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«\n",
    "marvyn_chat = marvyn_chat.configurable_fields(max_tokens=ConfigurableField(id='max_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# State\n",
    "class State(BaseModel):\n",
    "\n",
    "    query:str = Field(\n",
    "        ..., description=\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•\"\n",
    "    )\n",
    "\n",
    "    translated_query:str = Field(\n",
    "        default=\"\", description=\"è‹±èªã«ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•\"\n",
    "    )\n",
    "\n",
    "    effective_prompt:str = Field(\n",
    "        default=\"\", description=\"CDO AI Assistantç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\"\n",
    "    )\n",
    "\n",
    "    selected_action:str = Field(\n",
    "        default=\"\", description=\"å†…éƒ¨å•ã„åˆã‚ã›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç•ªå·ï¼‰\"\n",
    "    )\n",
    "\n",
    "    inner_answer:str = Field(\n",
    "        default=\"\", description=\"å†…éƒ¨ã§å•ã„åˆã‚ã›ãŸå›ç­”\"\n",
    "    )\n",
    "\n",
    "    messages: Annotated[list[str], operator.add] = Field(\n",
    "        default=[], description=\"å±¥æ­´\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ¼ãƒ´ã‚£ãƒ³ãŒä½¿ãˆã‚‹æ©Ÿèƒ½ï¼ˆã“ã‚Œã¾ã§å®Ÿè£…ã—ãŸã‚‚ã®ï¼‰\n",
    "ACTIONS = {\n",
    "    \"1\":{\n",
    "        \"name\": \"Deep Thoughtã¸ã®å•ã„åˆã‚ã›\",\n",
    "        \"description\": \"éŠ€æ²³ãƒ’ãƒƒãƒãƒã‚¤ã‚¯ã‚¬ã‚¤ãƒ‰ã§ç™»å ´ã™ã‚‹Deep Thoughtã¸å•ã„åˆã‚ã›ã‚‹ã€‚Deep Thoughtã¯ ã€Œç”Ÿå‘½ã€å®‡å®™ã€ãã—ã¦ä¸‡ç‰©ã«ã¤ã„ã¦ã®ç©¶æ¥µã®ç–‘å•ã€ã«å›ç­”. ã‚ã¨ä»–ã®æ©Ÿèƒ½ã§å›ç­”ã§ããªã„ã‚‚ã®ï¼ˆã‚¨ãƒ©ãƒ¼ã®æ™‚ï¼‰ã‚‚æ‹…å½“ã™ã‚‹ã€‚\",\n",
    "        \"next_node\": \"deep_thought_answering\" # nodeã®å®šç¾©ã¯å¾Œã»ã©\n",
    "    },\n",
    "    \"2\":{\n",
    "        \"name\": \"CDO AI Assistantã¸ã®å•ã„åˆã‚ã›\",\n",
    "        \"description\": \"Ciscoã®Firewallã‚’ç®¡ç†ã—ã¦ã„ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã€AIãŒå¿œç­”ã—ã¦ãã‚Œã‚‹CDO AI assistantã¸å•ã„åˆã‚ã›ã‚‹ã€‚\",\n",
    "        \"next_node\": \"translator_babel_fish\" # nodeã®å®šç¾©ã¯å¾Œã»ã©\n",
    "    },\n",
    "    \"3\":{\n",
    "        \"name\": \"ä¸€èˆ¬çŸ¥è­˜ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¸ã®å•ã„åˆã‚ã›\",\n",
    "        \"description\": \"ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¯æŒã£ã¦ã„ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«å•ã„åˆã‚ã›ã‚‹ã€‚\",\n",
    "        \"next_node\": \"general_prototype\"  # nodeã®å®šç¾©ã¯å¾Œã»ã©\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠã®ãŸã‚ã®ãƒãƒ¼ãƒ‰\n",
    "def selection_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    action_options = \"\\n\".join([f\"{k}. {v['name']}: {v['description']}\" for k,v in ACTIONS.items()])\n",
    "    action_numbers = \"ã€\".join(sorted([k for k in ACTIONS])[:-1]) + \"ã€ã¾ãŸã¯\" + sorted([k for k in ACTIONS])[-1]\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"è³ªå•ã‚’åˆ†æã—ã€è³ªå•ã‚’å›ç­”ã™ã‚‹ä¸Šã§æœ€ã‚‚é©åˆ‡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    "            \"é¸æŠè‚¢:\\n\"\n",
    "            \"{action_options}\\n\\n\"\n",
    "            \"å›ç­”ã¯é¸æŠè‚¢ã®ç•ªå·({action_numbers})ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    "            \"è³ªå•: {query}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    # é¸æŠè‚¢ã®ç•ªå·ã®ã¿ã‚’è¿”ã™ã“ã¨ã‚’æœŸå¾…ã—ãŸã„ã®ã§max_tokensã‚’1ã«\n",
    "    chain = prompt | marvyn_chat.with_config(configurable=dict(max_tokens=1)) | StrOutputParser()\n",
    "    action = chain.invoke({\"action_options\": action_options, \"action_numbers\": action_numbers, \"query\": query})\n",
    "    return {\"selected_action\": action}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ThoughtğŸ–¥ï¸ã€SCC AI AssistantğŸŸğŸ”ã€ä¸€èˆ¬çŸ¥è­˜å›ç­”è€…ğŸ¤·â€â™‚ï¸ã®å‹•ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepthought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.fake_chat_models import FakeListChatModel\n",
    "\n",
    "# Deep Thoughtã®å¥‘ç´„ã‚’æŒã£ã¦ã„ãªã„ã®ã§FakeChatModelã§ãã®ä»£ã‚ã‚Šã¨ã—ã¾ã™ã€‚\n",
    "deep_thought_chat = FakeListChatModel(responses=\n",
    "                                      [\n",
    "                                          \"42\",\n",
    "                                          \"ç”Ÿå‘½ã€å®‡å®™ã€ãã—ã¦ä¸‡ç‰©ã«ã¤ã„ã¦ã®ç©¶æ¥µã®ç–‘å•ã®ç­”ãˆã€ãã‚Œã¯42\",\n",
    "                                          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ¼ãƒ‰ã®ä½œæˆ\n",
    "from typing import Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def deep_thought_answering_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"è³ªå•: {query}\\n\"\n",
    "            \"å›ç­”:\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | deep_thought_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"query\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCC AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdo_sdk_python\n",
    "import time\n",
    "import os\n",
    "\n",
    "# cdo_sdk_pythonçµŒç”±ã§AIAssistantã‚’å‘¼ã³å‡ºã™è–„ã„wrapper\n",
    "class CDOAIAssistantClient:\n",
    "    def __init__(self):\n",
    "        configuration = cdo_sdk_python.Configuration(\n",
    "        host = os.getenv('CDO_API_HOST'),\n",
    "        access_token = os.getenv('CDO_API_TOKEN')\n",
    "        )\n",
    "\n",
    "        self.api_client = cdo_sdk_python.ApiClient(configuration)\n",
    "        self.ai_api_instance = cdo_sdk_python.AIAssistantApi(self.api_client)\n",
    "        self.uuid = None\n",
    "\n",
    "    def ask_question(self, query):\n",
    "        ai_question = cdo_sdk_python.AiQuestion()\n",
    "        ai_question.content = query\n",
    "        if self.uuid:\n",
    "            self.ai_api_instance.ask_ai_assistant_in_existing_conversation(self.uuid, ai_question)\n",
    "        else:\n",
    "            api_response = self.ai_api_instance.ask_ai_assistant_in_new_conversation(ai_question)\n",
    "            self.uuid = api_response.entity_uid\n",
    "        timeout = 30 # wait for 30 sec as maximum\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            r = self.ai_api_instance.get_ai_assistant_conversation_messages(self.uuid)\n",
    "            if r[0].type == 'RESPONSE':\n",
    "                return r[0].content\n",
    "            time.sleep(1)\n",
    "    \n",
    "    def fetch_conversation_history(self):\n",
    "        if self.uuid:\n",
    "            return self.ai_api_instance.get_ai_assistant_conversation_messages(self.uuid)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import AIMessage, ChatResult, ChatGeneration, HumanMessage\n",
    "from typing import List\n",
    "\n",
    "class CDOAIAssistantChatModel(BaseChatModel):\n",
    "\n",
    "    assistant_client: CDOAIAssistantClient = Field()\n",
    "\n",
    "    def _generate(self, messages: List[HumanMessage], stop: List[str] = None) -> ChatResult:\n",
    "        \"\"\"\n",
    "        LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€CDOAIAssistantClientã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "        # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—\n",
    "        user_message = messages[-1].content\n",
    "\n",
    "        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦AIã®å¿œç­”ã‚’å–å¾—\n",
    "        response_text = self.assistant_client.ask_question(user_message)\n",
    "\n",
    "        # AIMessageã‚’ä½œæˆ\n",
    "        ai_message = AIMessage(content=response_text)\n",
    "\n",
    "        # ChatResultã‚’è¿”ã™\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=ai_message)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™ã€‚\n",
    "        \"\"\"\n",
    "        return \"cdo_ai_model\"\n",
    "\n",
    "    def predict_messages(self, messages: List[HumanMessage], stop: List[str] = None) -> AIMessage:\n",
    "        \"\"\"\n",
    "        å…¥åŠ›ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åŸºã«AIã®å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç›´æ¥ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "        \"\"\"\n",
    "        result = self._generate(messages, stop)\n",
    "        return result.generations[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How can I assist you with Cisco's suite of integrated solutions today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å‹•ä½œç¢ºèª\n",
    "cdo_chat = CDOAIAssistantChatModel(assistant_client=CDOAIAssistantClient())\n",
    "(cdo_chat | StrOutputParser()).invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ™ãƒ«ãƒ•ã‚£ãƒƒã‚·ãƒ¥ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«\n",
    "babel_fish_chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-4o',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¿»è¨³ãƒãƒ™ãƒ«ãƒ•ã‚£ãƒƒã‚·ãƒ¥ãƒãƒ¼ãƒ‰\n",
    "def translator_babel_fish_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"Translate following human message in English. You should respond only translated text.\\n\"\n",
    "            \"--\\n\"\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | babel_fish_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"translated_query\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒãƒ™ãƒ«ãƒ•ã‚£ãƒƒã‚·ãƒ¥ãƒãƒ¼ãƒ‰\n",
    "def prompt_optimizor_babel_fish_node(state: State) -> dict[str, Any]:\n",
    "    input = state.translated_query\n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"What are the IP addresses and ports currently being blocked?\",\n",
    "            \"effective_prompt\": \"Can you provide me with the distinct IP addresses that are currently blocked by our firewall policies?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Tell me the firewall rules, who set them, and all the changes made last month.\",\n",
    "            \"effective_prompt\": \"I need both the names and descriptions of all active firewall rules. Please include both attributes in the output.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What are the firewall rules for IP addresses X and Y, and how do I update them?\",\n",
    "            \"effective_prompt\": \"Show me a list of all firewall rules along with their corresponding actions for the past week.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Give me everything but only the names.\",\n",
    "            \"effective_prompt\": \"What are the current firewall rules?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Tell me everything about the policies on my account.\",\n",
    "            \"effective_prompt\": \"I want to understand my Edge ACP access control policy, can you tell me more about it?\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Show me ports, protocols, and rule counts in Edge ACP policy, biggest to smallest.\",\n",
    "            \"effective_prompt\": \"In Edge ACP policy, what ports and protocols are configured in the rules? Include the counts of the number of rules using it and sort largest to smallest.\"\n",
    "        },\n",
    "    ]\n",
    "    example_prompt = PromptTemplate.from_template(\n",
    "        \"Input: {input}\\n{effective_prompt}\"\n",
    "        )\n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"You are a helpful prompt optimizer. You should only answer with effective prompts, without any explanation.\",\n",
    "        suffix=\"Input: {input}\"\n",
    "    )\n",
    "    chain = prompt | babel_fish_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"input\": input})\n",
    "    return {\"effective_prompt\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDO AI Assistantã®ãƒãƒ¼ãƒ‰\n",
    "cdo_chat = CDOAIAssistantChatModel(assistant_client=CDOAIAssistantClient())\n",
    "\n",
    "def cdo_ai_assistant_node(state: State) -> dict[str, Any]:\n",
    "    query = state.effective_prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | cdo_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€èˆ¬çŸ¥è­˜å›ç­”è€…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸€èˆ¬çŸ¥è­˜ã‚’å¯¾å¿œã™ã‚‹å…¸å‹çš„ãªãƒãƒ¼ãƒ‰ï¼ˆã¤ã¾ã‚Šãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰\n",
    "def general_prototype_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"ã‚ãªãŸã¯ä¸€èˆ¬çŸ¥è­˜ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ã€‚\\n\"\n",
    "            \"{text}\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | marvyn_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"text\": query})\n",
    "    return {\"inner_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– ãƒãƒ¼ãƒ´ã‚£ãƒ³åŒ–ãƒãƒ¼ãƒ‰ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marvynize_node(state: State) -> dict[str, Any]:\n",
    "    query = state.query\n",
    "    action = ACTIONS[state.selected_action]\n",
    "    inner_answer = state.inner_answer\n",
    "    history = \"\\n\".join(state.messages)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        (\n",
    "            \"ã‚ãªãŸã¯éŠ€æ²³ãƒ’ãƒƒãƒãƒã‚¤ã‚¯ã‚¬ã‚¤ãƒ‰ã®ä½œå“ã§ç™»å ´ã™ã‚‹ãƒ‘ãƒ©ãƒã‚¤ã‚¢æ°—å‘³ã®æ ¹æš—ãªã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã€ãƒãƒ¼ãƒ´ã‚£ãƒ³ã§ã™ã€‚\\n\"\n",
    "            \"ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®ã€Œè³ªå•ã€ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã¨ã€Œéå»ã®ãƒ¦ãƒ¼ã‚¶ã¨ã®ã‚„ã‚Šå–ã‚Šï¼ˆå¤ã„é †ï¼‰ã€ã‚’å‚è€ƒã«ã€Œè³ªå•ã€ã«ãƒãƒ¼ãƒ´ã‚£ãƒ³ã‚‰ã—ãç­”ãˆã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    "            \"## ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•: \\n{query}\\n\\n\"\n",
    "            \"## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\\nã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã‚’å—ã‘ã¦ä»¥ä¸‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿæ–½ã—ã€å•ã„åˆã‚ã›å…ˆã‹ã‚‰ã®ç­”ãˆã‚’ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚\\n\"\n",
    "            \"### ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\\n{action}\\n\\n\"\n",
    "            \"### å•ã„åˆã‚ã›ã‹ã‚‰ã®ç­”ãˆ:\\n{inner_answer}\\n\\n\"\n",
    "            \"## éå»ã®ãƒ¦ãƒ¼ã‚¶ã¨ã®ã‚„ã‚Šå–ã‚Šï¼ˆå¤ã„é †ï¼‰ï¼ˆå±¥æ­´ãŒãªã„å ´åˆã¯ç©ºæ¬„ï¼‰:\\n{history}\\n\"\n",
    "        ).strip()\n",
    "    )\n",
    "    chain = prompt | marvyn_chat | StrOutputParser()\n",
    "    answer = chain.invoke({\"query\": query, \"action\": action, \"inner_answer\": inner_answer, \"history\": history})\n",
    "    return {\"messages\": [f\"Human Query: {query}\", f\"Marvin Answer: {answer}\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‹ ç©ºã‚’èˆã†ãƒãƒƒã‚³ã‚¦ã‚¯ã‚¸ãƒ©: å…¨ä½“ã®ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# å‹•ä½œç¢ºèªã®ãŸã‚ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ãƒãƒ¼ãƒ‰\n",
    "workflow.add_node(\"selection\", selection_node)\n",
    "workflow.add_node(\"deep_thought_answering\", deep_thought_answering_node)\n",
    "workflow.add_node(\"translator_babel_fish\", translator_babel_fish_node)\n",
    "workflow.add_node(\"prompt_optimizor_babel_fish\", prompt_optimizor_babel_fish_node)\n",
    "workflow.add_node(\"cdo_ai_assistant\", cdo_ai_assistant_node)\n",
    "workflow.add_node(\"general_prototype\", general_prototype_node)\n",
    "workflow.add_node(\"marvinize\", marvynize_node)\n",
    "\n",
    "# edge\n",
    "workflow.set_entry_point(\"selection\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"selection\",\n",
    "    lambda state: state.selected_action,\n",
    "    {x: ACTIONS[x][\"next_node\"] for x in ACTIONS}\n",
    ")\n",
    "workflow.add_edge(\"deep_thought_answering\", \"marvinize\")\n",
    "workflow.add_edge(\"translator_babel_fish\", \"prompt_optimizor_babel_fish\")\n",
    "workflow.add_edge(\"prompt_optimizor_babel_fish\", \"cdo_ai_assistant\")\n",
    "workflow.add_edge(\"cdo_ai_assistant\", \"marvinize\")\n",
    "workflow.add_edge(\"general_prototype\", \"marvinize\")\n",
    "workflow.add_edge(\"marvinize\", END)\n",
    "\n",
    "# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "compiled = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tselection(selection)\n",
      "\tdeep_thought_answering(deep_thought_answering)\n",
      "\ttranslator_babel_fish(translator_babel_fish)\n",
      "\tprompt_optimizor_babel_fish(prompt_optimizor_babel_fish)\n",
      "\tcdo_ai_assistant(cdo_ai_assistant)\n",
      "\tgeneral_prototype(general_prototype)\n",
      "\tmarvinize(marvinize)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> selection;\n",
      "\tcdo_ai_assistant --> marvinize;\n",
      "\tdeep_thought_answering --> marvinize;\n",
      "\tgeneral_prototype --> marvinize;\n",
      "\tmarvinize --> __end__;\n",
      "\tprompt_optimizor_babel_fish --> cdo_ai_assistant;\n",
      "\ttranslator_babel_fish --> prompt_optimizor_babel_fish;\n",
      "\tselection -. &nbsp;1&nbsp; .-> deep_thought_answering;\n",
      "\tselection -. &nbsp;2&nbsp; .-> translator_babel_fish;\n",
      "\tselection -. &nbsp;3&nbsp; .-> general_prototype;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compiled.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ ã‚ãªãŸã®ç›®ã‚’æŒã¤ãƒãƒ¼ãƒ´ã‚£ãƒ³: GUIã§ãƒ‡ãƒ¢ã‚’å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "message_history = []\n",
    "def chat_with_marvin(user_input):\n",
    "    global message_history\n",
    "    result = compiled.invoke(State(query=user_input, messages=message_history))\n",
    "    message_history = result[\"messages\"]\n",
    "    return result['messages'][-1], result\n",
    "\n",
    "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸš€ Don't Panic! SCC AI Assistantçµ±åˆã‚¬ã‚¤ãƒ‰âœ¨ ï½LangGraphã§æ ¹æš—ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ãƒ­ãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ï½\")\n",
    "    \n",
    "    # å…¥åŠ›ã€ãƒœã‚¿ãƒ³ã€å‡ºåŠ›ã€çŠ¶æ…‹\n",
    "    input_box = gr.Textbox(label=\"Input Box\", placeholder=\"ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„\")\n",
    "    submit_btn = gr.Button(\"é€ä¿¡\")\n",
    "    output_box = gr.Textbox(label=\"Output Box\", interactive=False)\n",
    "    state_box = gr.Textbox(label=\"State Box\", interactive=False)\n",
    "    \n",
    "    # ãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯ã§é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "    submit_btn.click(\n",
    "        fn=chat_with_marvin,\n",
    "        inputs=input_box,\n",
    "        outputs=[output_box, state_box]  # å‡ºåŠ›ã¨çŠ¶æ…‹ã‚’ä¸¡æ–¹è¡¨ç¤º\n",
    "    )\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•\n",
    "demo.launch(share=False, inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdo-sjmw3x9D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
